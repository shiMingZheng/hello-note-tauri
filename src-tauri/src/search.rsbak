// src-tauri/src/search.rs
// Tantivy 全文搜索引擎模块 - 修复版

use anyhow::{Context, Result};
use once_cell::sync::Lazy;
use serde::{Deserialize, Serialize};
use std::fs;
use std::path::Path;
use std::sync::Arc;
use tantivy::collector::TopDocs;
use tantivy::directory::MmapDirectory;
use tantivy::query::QueryParser;
use tantivy::schema::{Field, IndexRecordOption, Schema, TextFieldIndexing, TextOptions, STORED, STRING};
use tantivy::tokenizer::{LowerCaser, RemoveLongFilter, TextAnalyzer};
use tantivy::{doc, Index, IndexWriter, ReloadPolicy, TantivyDocument};
use tantivy::schema::Value;

// 使用全局静态 Jieba 分词器
static JIEBA_TOKENIZER: Lazy<tantivy_jieba::JiebaTokenizer> = Lazy::new(|| {
    tantivy_jieba::JiebaTokenizer {}
});

// 搜索结果结构体
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SearchResult {
    pub path: String,
    pub title: String,
    pub snippet: String,
}

// Schema 字段定义
pub struct SchemaFields {
    pub path: Field,
    pub title: Field,
    pub content: Field,
}

/// 创建 Tantivy 索引的 Schema
pub fn build_schema() -> (Schema, SchemaFields) {
    let mut schema_builder = Schema::builder();
    
    // 路径字段：存储但不索引（用作唯一标识）
    let path = schema_builder.add_text_field("path", STRING | STORED);
    
    // 标题和内容字段：使用中文分词器
    let text_field_indexing = TextFieldIndexing::default()
        .set_tokenizer("jieba")
        .set_index_option(IndexRecordOption::WithFreqsAndPositions);
    let text_options = TextOptions::default()
        .set_indexing_options(text_field_indexing)
        .set_stored();
    
    let title = schema_builder.add_text_field("title", text_options.clone());
    let content = schema_builder.add_text_field("content", text_options);
    
    let schema = schema_builder.build();
    let fields = SchemaFields { path, title, content };
    
    (schema, fields)
}

/// 初始化或打开 Tantivy 索引
pub fn initialize_index(base_path: &Path) -> Result<Arc<Index>> {
    let index_path = base_path.join(".cheetah_index");
    
    // 确保索引目录存在
    if !index_path.exists() {
        fs::create_dir_all(&index_path)
            .with_context(|| format!("创建索引目录失败: {}", index_path.display()))?;
    }
    
    let (schema, _) = build_schema();
    
    // 尝试打开或创建索引
    let index = if index_path.join("meta.json").exists() {
        let dir = MmapDirectory::open(&index_path)
            .with_context(|| format!("打开索引目录失败: {}", index_path.display()))?;
        Index::open(dir)
            .with_context(|| "打开现有索引失败")?
    } else {
        let dir = MmapDirectory::open(&index_path)
            .with_context(|| format!("创建索引目录失败: {}", index_path.display()))?;
        Index::create_in_dir(&index_path, schema.clone()).unwrap()
           // .with_context(|| "创建新索引失败")?
    };
    
    // 注册 Jieba 分词器 - 使用 tantivy 0.24.1 的正确方式
    let analyzer = TextAnalyzer::builder(JIEBA_TOKENIZER.clone())
        .filter(RemoveLongFilter::limit(40))
        .filter(LowerCaser)
        .build();
    
    index.tokenizers().register("jieba", analyzer);
    
    println!("✅ Tantivy 索引已初始化: {}", index_path.display());
    
    Ok(Arc::new(index))
}

/// 递归索引所有 Markdown 文件
pub fn index_documents(index: &Index, base_path: &Path) -> Result<()> {
    let (_, fields) = build_schema();
    
    // 创建索引写入器（使用 50MB 的缓冲区）
    let mut index_writer: IndexWriter<TantivyDocument> = index
        .writer(50_000_000)
        .with_context(|| "创建索引写入器失败")?;
    
    // 清空现有索引
    index_writer.delete_all_documents()
        .with_context(|| "清空索引失败")?;
    
    // 递归索引文件
    let mut count = 0;
    index_directory_recursive(&mut index_writer, &fields, base_path, &mut count)?;
    
    // 提交索引
    index_writer.commit()
        .with_context(|| "提交索引失败")?;
    
    println!("✅ 文件索引完成，共索引 {} 个文件", count);
    Ok(())
}

/// 递归遍历目录并索引文件
fn index_directory_recursive(
    writer: &mut IndexWriter<TantivyDocument>,
    fields: &SchemaFields,
    dir: &Path,
    count: &mut usize,
) -> Result<()> {
    let entries = fs::read_dir(dir)
        .with_context(|| format!("读取目录失败: {}", dir.display()))?;
    
    for entry in entries {
        let entry = entry?;
        let path = entry.path();
        
        // 跳过隐藏文件和索引目录
        if let Some(name) = path.file_name() {
            let name_str = name.to_string_lossy();
            if name_str.starts_with('.') {
                continue;
            }
        }
        
        let metadata = entry.metadata()?;
        
        if metadata.is_dir() {
            // 递归处理子目录
            index_directory_recursive(writer, fields, &path, count)?;
        } else if path.extension().and_then(|s| s.to_str()) == Some("md") {
            // 索引 Markdown 文件
            index_single_document(writer, fields, &path)?;
            *count += 1;
        }
    }
    
    Ok(())
}

/// 索引单个文档
pub fn index_single_document(
    writer: &mut IndexWriter<TantivyDocument>,
    fields: &SchemaFields,
    file_path: &Path,
) -> Result<()> {
    let content = fs::read_to_string(file_path)
        .with_context(|| format!("读取文件失败: {}", file_path.display()))?;
    
    let title = extract_title_from_content(&content)
        .unwrap_or_else(|| {
            file_path
                .file_stem()
                .and_then(|s| s.to_str())
                .unwrap_or("无标题")
                .to_string()
        });
    
    let path_str = file_path.to_string_lossy().to_string();
    
    // 先删除旧文档（如果存在）
    let path_term = tantivy::Term::from_field_text(fields.path, &path_str);
    writer.delete_term(path_term);
    
    // 添加新文档
    let doc = doc!(
        fields.path => path_str,
        fields.title => title,
        fields.content => content
    );
    
    writer.add_document(doc)
        .with_context(|| format!("索引文档失败: {}", file_path.display()))?;
    
    Ok(())
}

/// 更新单个文件的索引
pub fn update_document_index(index: &Index, file_path: &Path) -> Result<()> {
    let (_, fields) = build_schema();
    
    let mut writer: IndexWriter<TantivyDocument> = index.writer(5_000_000)?;
    index_single_document(&mut writer, &fields, file_path)?;
    writer.commit()?;
    
    Ok(())
}

/// 从索引中删除文档
pub fn delete_document(index: &Index, file_path: &str) -> Result<()> {
    let (_, fields) = build_schema();
    
    let mut writer: IndexWriter<TantivyDocument> = index.writer(5_000_000)
        .with_context(|| "创建索引写入器失败")?;
    
    let path_term = tantivy::Term::from_field_text(fields.path, file_path);
    writer.delete_term(path_term);
    
    writer.commit()
        .with_context(|| format!("删除文档失败: {}", file_path))?;
    
    println!("✅ 已从索引中删除: {}", file_path);
    Ok(())
}

/// 执行搜索
pub fn search(index: &Index, query: &str) -> Result<Vec<SearchResult>> {
    if query.trim().is_empty() {
        return Ok(Vec::new());
    }
    
    let (_, fields) = build_schema();
    
    // 创建索引读取器
    let reader = index
        .reader_builder()
        .reload_policy(ReloadPolicy::OnCommitWithDelay)
        .try_into()
        .with_context(|| "创建索引读取器失败")?;
    
    let searcher = reader.searcher();
    
    // 创建查询解析器（同时搜索标题和内容）
    let query_parser = QueryParser::for_index(
        index,
        vec![fields.title, fields.content],
    );
    
    // 解析查询
    let parsed_query = query_parser
        .parse_query(query)
        .with_context(|| format!("解析查询失败: {}", query))?;
    
    // 执行搜索（获取前 10 个结果）
    let top_docs = searcher
        .search(&parsed_query, &TopDocs::with_limit(10)).unwrap();
        //.with_context(|| "执行搜索失败")?;
    
    // 处理搜索结果
    let mut results = Vec::new();
    for (_, doc_address) in top_docs {
        // let doc: TantivyDocument = searcher
        //     .doc(doc_address)
        //     .with_context(|| "获取文档失败")?;
        
        // 提取字段值
        // let path = doc
        //     .get_first(fields.path).unwrap()
        //     .and_then(|v| v.as_str())
        //     .unwrap_or("")
        //     .to_string();
        let doc: TantivyDocument = searcher.doc(doc_address).unwrap();
       // let doc: TantivyDocument = searcher.doc(doc_address).
         let val = doc.get_first(fields.path).unwrap();
        let path = val.as_str().unwrap_or_default().to_string();
        
        // let title = doc
        //     .get_first(fields.title)
        //     .and_then(|v| v.as_text())
        //     .unwrap_or("无标题")
        //     .to_string();

        let title_val = doc.get_first(fields.title).unwrap();
         let title = title_val.as_str().unwrap_or_default().to_string();
        
        // 生成简单预览（前 150 个字符）

        // let snippet = doc
        //     .get_first(fields.content)
        //     .and_then(|v| v.as_text())
        //     .map(|s| {
        //         let preview = if s.len() > 150 {
        //             format!("{}...", &s[..150])
        //         } else {
        //             s.to_string()
        //         };
        //         highlight_keywords(&preview, query)
        //     })
        //     .unwrap_or_else(|| "无预览内容".to_string());
        let snippet_val = doc.get_first(fields.title).unwrap().as_str();
        let snippet = snippet_val .map(|s| {
                    let preview = if s.len() > 150 {
                        format!("{}...", &s[..150])
                    } else {
                        s.to_string()
                    };
                    highlight_keywords(&preview, query)
                })
                .unwrap_or_else(|| "无预览内容".to_string());
        
        results.push(SearchResult {
            path,
            title,
            snippet,
        });
    }
    
    Ok(results)
}

/// 从内容中提取标题
fn extract_title_from_content(content: &str) -> Option<String> {
    content
        .lines()
        .find(|line| line.starts_with("# "))
        .map(|line| line[2..].trim().to_string())
}

/// 高亮关键词
fn highlight_keywords(text: &str, query: &str) -> String {
    let mut result = text.to_string();
    let query_lower = query.to_lowercase();
    let text_lower = text.to_lowercase();
    
    if let Some(pos) = text_lower.find(&query_lower) {
        let end = pos + query.len();
        if end <= text.len() {
            let before = &text[..pos];
            let matched = &text[pos..end];
            let after = &text[end..];
            result = format!("{}<b>{}</b>{}", before, matched, after);
        }
    }
    
    result
}