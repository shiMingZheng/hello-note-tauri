// src-tauri/src/search.rs
// Tantivy å…¨æ–‡æœç´¢å¼•æ“æ¨¡å— - å†…å­˜ä¼˜åŒ–ç‰ˆ

use anyhow::{Context, Result};
use once_cell::sync::Lazy;
use serde::{Deserialize, Serialize};
use std::fs;
use std::path::Path;
use std::sync::Arc;
use tantivy::collector::TopDocs;
use tantivy::directory::MmapDirectory;
use tantivy::query::QueryParser;
// [ä¿®æ”¹] ç§»é™¤äº† `STORED` å’Œ `STRING` çš„ç›´æ¥å¯¼å…¥ï¼Œå› ä¸ºæˆ‘ä»¬ä¼šæ›´ç²¾ç¡®åœ°ä½¿ç”¨å®ƒä»¬
use tantivy::schema::{Field, IndexRecordOption, Schema, TextFieldIndexing, TextOptions};
use tantivy::tokenizer::{LowerCaser, RemoveLongFilter, TextAnalyzer};
use tantivy::{doc, Index, IndexWriter, ReloadPolicy, TantivyDocument};
//è§£å†³è¿™ä¸ªé—®é¢˜v.as_str())^^^^^^ method not found in `CompactDocValue<'_>`
use tantivy::schema::Value;

// (å…¨å±€é™æ€ Jieba åˆ†è¯å™¨ä¿æŒä¸å˜)
static JIEBA_TOKENIZER: Lazy<tantivy_jieba::JiebaTokenizer> = Lazy::new(|| {
    tantivy_jieba::JiebaTokenizer {}
});

// (æœç´¢ç»“æœç»“æ„ä½“ä¿æŒä¸å˜)
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SearchResult {
    pub path: String,
    pub title: String,
    pub snippet: String,
}

// (Schema å­—æ®µå®šä¹‰ä¿æŒä¸å˜)
pub struct SchemaFields {
    pub path: Field,
    pub title: Field,
    pub content: Field,
}

/// åˆ›å»º Tantivy ç´¢å¼•çš„ Schema
pub fn build_schema() -> (Schema, SchemaFields) {
    let mut schema_builder = Schema::builder();
    
    // è·¯å¾„å­—æ®µï¼šå¿…é¡»è¢«å­˜å‚¨ï¼Œå®ƒæ˜¯æ–‡æ¡£çš„å”¯ä¸€ID
    let path = schema_builder.add_text_field("path", tantivy::schema::STRING | tantivy::schema::STORED);
    
    // åˆ›å»ºä¸€ä¸ªé€šç”¨çš„ä¸­æ–‡åˆ†è¯ç´¢å¼•é€‰é¡¹
    let text_field_indexing = TextFieldIndexing::default()
        .set_tokenizer("jieba")
        .set_index_option(IndexRecordOption::WithFreqsAndPositions);
    
    // æ ‡é¢˜å­—æ®µï¼šéœ€è¦è¢«ç´¢å¼•å’Œå­˜å‚¨ï¼ˆç”¨äºåœ¨ç»“æœåˆ—è¡¨ä¸­ç›´æ¥æ˜¾ç¤ºï¼‰
    let title_options = TextOptions::default()
        .set_indexing_options(text_field_indexing.clone())
        .set_stored();
    let title = schema_builder.add_text_field("title", title_options);
    
    // [æ ¸å¿ƒä¼˜åŒ–] å†…å®¹å­—æ®µï¼šåªç´¢å¼•ï¼Œä¸å­˜å‚¨ï¼
    // è¿™å°†æå¤§åœ°å‡å°ç´¢å¼•æ–‡ä»¶çš„å¤§å°å’Œå†…å­˜å ç”¨
    let content_options = TextOptions::default()
        .set_indexing_options(text_field_indexing);
    let content = schema_builder.add_text_field("content", content_options);
    
    let schema = schema_builder.build();
    let fields = SchemaFields { path, title, content };
    
    (schema, fields)
}

/// (åˆå§‹åŒ–æˆ–æ‰“å¼€ Tantivy ç´¢å¼•å‡½æ•°ä¿æŒä¸å˜)
pub fn initialize_index(base_path: &Path) -> Result<Arc<Index>> {
    let index_path = base_path.join(".cheetah_index");
    
    if !index_path.exists() {
        fs::create_dir_all(&index_path)
            .with_context(|| format!("åˆ›å»ºç´¢å¼•ç›®å½•å¤±è´¥: {}", index_path.display()))?;
    }
    
    let (schema, _) = build_schema();
    
    let index = if index_path.join("meta.json").exists() {
        let dir = MmapDirectory::open(&index_path)
            .with_context(|| format!("æ‰“å¼€ç´¢å¼•ç›®å½•å¤±è´¥: {}", index_path.display()))?;
        Index::open(dir)
            .with_context(|| "æ‰“å¼€ç°æœ‰ç´¢å¼•å¤±è´¥")?
    } else {
        let dir = MmapDirectory::open(&index_path)
            .with_context(|| format!("åˆ›å»ºç´¢å¼•ç›®å½•å¤±è´¥: {}", index_path.display()))?;
    
		Index::open_or_create(dir, schema.clone()).with_context(|| "åˆ›å»ºç´¢å¼•å¤±è´¥")?
    };
    
    let analyzer = TextAnalyzer::builder(JIEBA_TOKENIZER.clone())
        .filter(RemoveLongFilter::limit(40))
        .filter(LowerCaser)
        .build();
    
    index.tokenizers().register("jieba", analyzer);
    
    println!("âœ… Tantivy ç´¢å¼•å·²åˆå§‹åŒ–: {}", index_path.display());
    
    Ok(Arc::new(index))
}


/// (é€’å½’ç´¢å¼•æ‰€æœ‰ Markdown æ–‡ä»¶å‡½æ•°ä¿æŒä¸å˜)
pub fn index_documents(index: &Index, base_path: &Path) -> Result<()> {
    let (_, fields) = build_schema();
    
    let mut index_writer: IndexWriter = index
        .writer(50_000_000)
        .with_context(|| "åˆ›å»ºç´¢å¼•å†™å…¥å™¨å¤±è´¥")?;
    
    index_writer.delete_all_documents()
        .with_context(|| "æ¸…ç©ºç´¢å¼•å¤±è´¥")?;
    
    let mut count = 0;
    index_directory_recursive(&mut index_writer, &fields, base_path, &mut count)?;
    
    index_writer.commit()
        .with_context(|| "æäº¤ç´¢å¼•å¤±è´¥")?;
    
    println!("âœ… æ–‡ä»¶ç´¢å¼•å®Œæˆï¼Œå…±ç´¢å¼• {} ä¸ªæ–‡ä»¶", count);
    Ok(())
}

/// (é€’å½’éå†ç›®å½•å¹¶ç´¢å¼•æ–‡ä»¶å‡½æ•°ä¿æŒä¸å˜)
fn index_directory_recursive(
    writer: &mut IndexWriter,
    fields: &SchemaFields,
    dir: &Path,
    count: &mut usize,
) -> Result<()> {
    let entries = fs::read_dir(dir)
        .with_context(|| format!("è¯»å–ç›®å½•å¤±è´¥: {}", dir.display()))?;
    
    for entry in entries {
        let entry = entry?;
        let path = entry.path();
        
        if let Some(name) = path.file_name() {
            let name_str = name.to_string_lossy();
            if name_str.starts_with('.') {
                continue;
            }
        }
        
        let metadata = entry.metadata()?;
        
        if metadata.is_dir() {
            index_directory_recursive(writer, fields, &path, count)?;
        } else if path.extension().and_then(|s| s.to_str()) == Some("md") {
            index_single_document(writer, fields, &path)?;
            *count += 1;
        }
    }
    
    Ok(())
}

/// (ç´¢å¼•å•ä¸ªæ–‡æ¡£å‡½æ•°ä¿æŒä¸å˜)
pub fn index_single_document(
    writer: &mut IndexWriter,
    fields: &SchemaFields,
    file_path: &Path,
) -> Result<()> {
    let content = fs::read_to_string(file_path)
        .with_context(|| format!("è¯»å–æ–‡ä»¶å¤±è´¥: {}", file_path.display()))?;
    
    let title = extract_title_from_content(&content)
        .unwrap_or_else(|| {
            file_path
                .file_stem()
                .and_then(|s| s.to_str())
                .unwrap_or("æ— æ ‡é¢˜")
                .to_string()
        });
    
    let path_str = file_path.to_string_lossy().to_string();
    
    let path_term = tantivy::Term::from_field_text(fields.path, &path_str);
    writer.delete_term(path_term);
    
    let doc = doc!(
        fields.path => path_str,
        fields.title => title,
        fields.content => content
    );
    
    writer.add_document(doc)
        .with_context(|| format!("ç´¢å¼•æ–‡æ¡£å¤±è´¥: {}", file_path.display()))?;
    
    Ok(())
}

/// (æ›´æ–°å•ä¸ªæ–‡ä»¶çš„ç´¢å¼•å‡½æ•°ä¿æŒä¸å˜)
pub fn update_document_index(index: &Index, file_path: &Path) -> Result<()> {
    let (_, fields) = build_schema();
    
    let mut writer: IndexWriter = index.writer(5_000_000)?;
    index_single_document(&mut writer, &fields, file_path)?;
    writer.commit()?;
    
    Ok(())
}

/// (ä»ç´¢å¼•ä¸­åˆ é™¤æ–‡æ¡£å‡½æ•°ä¿æŒä¸å˜)
pub fn delete_document(index: &Index, file_path: &str) -> Result<()> {
    let (_, fields) = build_schema();
    
    let mut writer: IndexWriter = index.writer(5_000_000)
        .with_context(|| "åˆ›å»ºç´¢å¼•å†™å…¥å™¨å¤±è´¥")?;
    
    let path_term = tantivy::Term::from_field_text(fields.path, file_path);
    writer.delete_term(path_term);
    
    writer.commit()
        .with_context(|| format!("åˆ é™¤æ–‡æ¡£å¤±è´¥: {}", file_path))?;
    
    println!("âœ… å·²ä»ç´¢å¼•ä¸­åˆ é™¤: {}", file_path);
    Ok(())
}

/// (å®‰å…¨åœ°æˆªå–å­—ç¬¦ä¸²å‡½æ•°ä¿æŒä¸å˜)
fn safe_truncate(s: &str, max_chars: usize) -> String {
    s.chars().take(max_chars).collect()
}


/// æ‰§è¡Œæœç´¢ - [æ ¸å¿ƒä¼˜åŒ–]
pub fn search(index: &Index, query: &str) -> Result<Vec<SearchResult>> {
    if query.trim().is_empty() {
        return Ok(Vec::new());
    }
    
    let (_, fields) = build_schema();
    
    let reader = index
        .reader_builder()
        .reload_policy(ReloadPolicy::OnCommitWithDelay)
        .try_into()
        .with_context(|| "åˆ›å»ºç´¢å¼•è¯»å–å™¨å¤±è´¥")?;
    
    let searcher = reader.searcher();
    
    let query_parser = QueryParser::for_index(
        index,
        vec![fields.title, fields.content],
    );
    
    let parsed_query = query_parser
        .parse_query(query)
        .with_context(|| format!("è§£ææŸ¥è¯¢å¤±è´¥: {}", query))?;
    
    let top_docs = searcher
        .search(&parsed_query, &TopDocs::with_limit(10))
        .with_context(|| "æ‰§è¡Œæœç´¢å¤±è´¥")?;
    
    let mut results = Vec::new();
    
    for (_score, doc_address) in top_docs {
         // å®‰å…¨è·å–æ–‡æ¡£
        let retrieved_doc: TantivyDocument = match searcher.doc(doc_address) {
            Ok(doc) => doc,
            Err(e) => {
                eprintln!("è·å–æ–‡æ¡£å¤±è´¥: {}", e);
                continue;
            }
        };
        
        // å®‰å…¨æå–è·¯å¾„
        let path = retrieved_doc
            .get_first(fields.path)
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .to_string();
		
        let title = retrieved_doc.get_first(fields.title).and_then(|v| v.as_str()).unwrap_or("æ— æ ‡é¢˜").to_string();
        
        if path.is_empty() {
            continue;
        }
        
        // [æ ¸å¿ƒä¼˜åŒ–] æŒ‰éœ€ä»ç£ç›˜è¯»å–æ–‡ä»¶å†…å®¹æ¥ç”Ÿæˆæ‘˜è¦
        // æˆ‘ä»¬ä¸å†ä»ç´¢å¼•ä¸­è¯»å–å†…å®¹ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»ä¸å†å­˜å‚¨å®ƒäº†
        let content = fs::read_to_string(&path).unwrap_or_else(|_| "".to_string());
        
        // ç”Ÿæˆæ‘˜è¦
        let snippet = if content.chars().count() > 150 {
            format!("{}...", safe_truncate(&content, 150))
        } else {
            content
        };
        
        results.push(SearchResult {
            path,
            title,
            snippet,
        });
    }
    
    println!("ğŸ” æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {} ä¸ªç»“æœ", results.len());
    Ok(results)
}

/// (ä» Markdown å†…å®¹ä¸­æå–æ ‡é¢˜å‡½æ•°ä¿æŒä¸å˜)
fn extract_title_from_content(content: &str) -> Option<String> {
    for line in content.lines() {
        let trimmed = line.trim();
        if trimmed.starts_with("# ") {
            return Some(trimmed[2..].trim().to_string());
        }
    }
    None
}